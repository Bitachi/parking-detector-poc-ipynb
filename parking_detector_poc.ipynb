{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1lvv-BishD5_KUczcX2HixFRXzab1UTql",
      "authorship_tag": "ABX9TyPUscL2tuJ/RdfO+FmTGteB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bitachi/parking-detector-poc-ipynb/blob/main/parking_detector_poc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uN7le7YxN-xp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6342c367-0d96-4ae8-8d5f-bc2140c52fdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m101.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "#必要なライブラリをインストール\n",
        "!pip install ultralytics deep_sort_realtime opencv-python-headless --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#検証対象の動画ファイルおよびYOLOモデルをダウンロード\n",
        "!wget \"https://parking-detection-source.s3.ap-northeast-1.amazonaws.com/fixedPointCamera.mp4\" -O /content/fixedPointCameraAkihabara.mp4\n",
        "!wget \"https://parking-detection-source.s3.ap-northeast-1.amazonaws.com/sapporo_2.mp4\" -O /content/fixedPointCameraSapporo.mp4\n",
        "!wget \"https://parking-detection-source.s3.ap-northeast-1.amazonaws.com/yolov8l.pt\" -O /content/yolov8l.pt\n",
        "!wget \"https://parking-detection-source.s3.ap-northeast-1.amazonaws.com/yolov8m.pt\" -O /content/yolov8m.pt\n",
        "!wget \"https://parking-detection-source.s3.ap-northeast-1.amazonaws.com/yolov8n.pt\" -O /content/yolov8n.pt\n",
        "!wget \"https://parking-detection-source.s3.ap-northeast-1.amazonaws.com/yolov5s.pt\" -O /content/yolov5s.pt"
      ],
      "metadata": {
        "id": "lUljXe3IPsF2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb964873-15fa-4749-8f3d-b0d1fa5dffea"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-01 12:16:19--  https://parking-detection-source.s3.ap-northeast-1.amazonaws.com/fixedPointCamera.mp4\n",
            "Resolving parking-detection-source.s3.ap-northeast-1.amazonaws.com (parking-detection-source.s3.ap-northeast-1.amazonaws.com)... 52.219.16.99, 3.5.156.242, 3.5.158.238, ...\n",
            "Connecting to parking-detection-source.s3.ap-northeast-1.amazonaws.com (parking-detection-source.s3.ap-northeast-1.amazonaws.com)|52.219.16.99|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 463496808 (442M) [video/mp4]\n",
            "Saving to: ‘/content/fixedPointCameraAkihabara.mp4’\n",
            "\n",
            "/content/fixedPoint 100%[===================>] 442.02M  20.2MB/s    in 24s     \n",
            "\n",
            "2025-06-01 12:16:44 (18.4 MB/s) - ‘/content/fixedPointCameraAkihabara.mp4’ saved [463496808/463496808]\n",
            "\n",
            "--2025-06-01 12:16:44--  https://parking-detection-source.s3.ap-northeast-1.amazonaws.com/sapporo_2.mp4\n",
            "Resolving parking-detection-source.s3.ap-northeast-1.amazonaws.com (parking-detection-source.s3.ap-northeast-1.amazonaws.com)... 52.219.136.143, 52.219.0.203, 3.5.159.117, ...\n",
            "Connecting to parking-detection-source.s3.ap-northeast-1.amazonaws.com (parking-detection-source.s3.ap-northeast-1.amazonaws.com)|52.219.136.143|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 55909204 (53M) [video/mp4]\n",
            "Saving to: ‘/content/fixedPointCameraSapporo.mp4’\n",
            "\n",
            "/content/fixedPoint 100%[===================>]  53.32M  16.5MB/s    in 3.9s    \n",
            "\n",
            "2025-06-01 12:16:48 (13.8 MB/s) - ‘/content/fixedPointCameraSapporo.mp4’ saved [55909204/55909204]\n",
            "\n",
            "--2025-06-01 12:16:48--  https://parking-detection-source.s3.ap-northeast-1.amazonaws.com/yolov8l.pt\n",
            "Resolving parking-detection-source.s3.ap-northeast-1.amazonaws.com (parking-detection-source.s3.ap-northeast-1.amazonaws.com)... 52.219.136.143, 52.219.0.203, 3.5.159.117, ...\n",
            "Connecting to parking-detection-source.s3.ap-northeast-1.amazonaws.com (parking-detection-source.s3.ap-northeast-1.amazonaws.com)|52.219.136.143|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 87769683 (84M) [binary/octet-stream]\n",
            "Saving to: ‘/content/yolov8l.pt’\n",
            "\n",
            "/content/yolov8l.pt 100%[===================>]  83.70M  19.0MB/s    in 5.5s    \n",
            "\n",
            "2025-06-01 12:16:55 (15.2 MB/s) - ‘/content/yolov8l.pt’ saved [87769683/87769683]\n",
            "\n",
            "--2025-06-01 12:16:55--  https://parking-detection-source.s3.ap-northeast-1.amazonaws.com/yolov8m.pt\n",
            "Resolving parking-detection-source.s3.ap-northeast-1.amazonaws.com (parking-detection-source.s3.ap-northeast-1.amazonaws.com)... 3.5.154.62, 3.5.156.186, 3.5.158.194, ...\n",
            "Connecting to parking-detection-source.s3.ap-northeast-1.amazonaws.com (parking-detection-source.s3.ap-northeast-1.amazonaws.com)|3.5.154.62|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 52136884 (50M) [binary/octet-stream]\n",
            "Saving to: ‘/content/yolov8m.pt’\n",
            "\n",
            "/content/yolov8m.pt 100%[===================>]  49.72M  15.4MB/s    in 3.4s    \n",
            "\n",
            "2025-06-01 12:16:59 (14.7 MB/s) - ‘/content/yolov8m.pt’ saved [52136884/52136884]\n",
            "\n",
            "--2025-06-01 12:16:59--  https://parking-detection-source.s3.ap-northeast-1.amazonaws.com/yolov8n.pt\n",
            "Resolving parking-detection-source.s3.ap-northeast-1.amazonaws.com (parking-detection-source.s3.ap-northeast-1.amazonaws.com)... 3.5.154.62, 3.5.156.186, 3.5.158.194, ...\n",
            "Connecting to parking-detection-source.s3.ap-northeast-1.amazonaws.com (parking-detection-source.s3.ap-northeast-1.amazonaws.com)|3.5.154.62|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6534387 (6.2M) [binary/octet-stream]\n",
            "Saving to: ‘/content/yolov8n.pt’\n",
            "\n",
            "/content/yolov8n.pt 100%[===================>]   6.23M  4.90MB/s    in 1.3s    \n",
            "\n",
            "2025-06-01 12:17:01 (4.90 MB/s) - ‘/content/yolov8n.pt’ saved [6534387/6534387]\n",
            "\n",
            "--2025-06-01 12:17:01--  https://parking-detection-source.s3.ap-northeast-1.amazonaws.com/yolov5s.pt\n",
            "Resolving parking-detection-source.s3.ap-northeast-1.amazonaws.com (parking-detection-source.s3.ap-northeast-1.amazonaws.com)... 52.219.17.38, 3.5.156.57, 3.5.154.21, ...\n",
            "Connecting to parking-detection-source.s3.ap-northeast-1.amazonaws.com (parking-detection-source.s3.ap-northeast-1.amazonaws.com)|52.219.17.38|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14808437 (14M) [binary/octet-stream]\n",
            "Saving to: ‘/content/yolov5s.pt’\n",
            "\n",
            "/content/yolov5s.pt 100%[===================>]  14.12M  8.07MB/s    in 1.7s    \n",
            "\n",
            "2025-06-01 12:17:04 (8.07 MB/s) - ‘/content/yolov5s.pt’ saved [14808437/14808437]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# モデルファイルと動画ファイルの存在確認\n",
        "model_file_path = \"/content/yolov8m.pt\"\n",
        "video_type = \"Akihabara\"\n",
        "#video_type = \"Sapporo\"\n",
        "video_file_path = \"/content/fixedPointCamera\" + video_type + \".mp4\"\n",
        "\n",
        "if not os.path.exists(model_file_path):\n",
        "    print(f\"エラー: モデルファイルが見つかりません: {model_file_path}\")\n",
        "    print(\"'/content/' ディレクトリに 'yolov8m.pt' をアップロードしてください。\")\n",
        "elif not os.path.exists(video_file_path):\n",
        "    print(f\"エラー: 動画ファイルが見つかりません: {video_file_path}\")\n",
        "    print(\"'/content/' ディレクトリに 'fixedPointCamera.mp4' をアップロードしてください。\")\n",
        "else:\n",
        "    print(\"モデルファイルと動画ファイルの準備ができました。\")"
      ],
      "metadata": {
        "id": "8ngncgbabn8j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de374fae-38fb-457a-9615-dd75a29db7ea"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "モデルファイルと動画ファイルの準備ができました。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "from ultralytics import YOLO\n",
        "import pandas as pd\n",
        "import time\n",
        "import os\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow # Colabでの画像表示用\n",
        "from google.colab import drive # Google Drive連携用\n",
        "import shutil # ファイル/ディレクトリ操作用\n",
        "\n",
        "# Google Driveをマウント\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Google Drive mounted successfully.\")\n",
        "    # Google Drive上の出力先ベースディレクトリを指定\n",
        "    # 例: MyDrive直下に \"ParkingDetectionOutput\" フォルダを作成し、その中に保存\n",
        "    # このディレクトリは事前に作成しておくか、コード内で作成処理を追加することも可能です。\n",
        "    DRIVE_OUTPUT_BASE_DIR = '/content/drive/MyDrive/ParkingDetectionOutput'\n",
        "    if not os.path.exists(DRIVE_OUTPUT_BASE_DIR):\n",
        "        os.makedirs(DRIVE_OUTPUT_BASE_DIR)\n",
        "        print(f\"Created Google Drive directory: {DRIVE_OUTPUT_BASE_DIR}\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to mount Google Drive: {e}\")\n",
        "    DRIVE_OUTPUT_BASE_DIR = None # Driveマウント失敗時はNoneに"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQFtv86kd6nQ",
        "outputId": "38e4e0c6-ac6f-4199-8080-c10cf72394d3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive mounted successfully.\n",
            "Created Google Drive directory: /content/drive/MyDrive/ParkingDetectionOutput\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# このセルを実行する前に、model_file_path, video_file_path, video_type が定義されている必要があります。\n",
        "# 例:\n",
        "model_file_path = \"/content/yolov8m.pt\"\n",
        "video_type = \"Akihabara\"\n",
        "#video_type = \"Sapporo\"\n",
        "video_file_path = \"/content/fixedPointCamera\" + video_type + \".mp4\"\n",
        "\n",
        "# 0. 設定値\n",
        "PROCESS_DURATION_SECONDS = 180\n",
        "FRAMES_PER_SECOND_TO_PROCESS = 1\n",
        "PARKING_THRESHOLD_SECONDS = 10\n",
        "CONFIDENCE_THRESHOLD = 0.15\n",
        "MOVEMENT_THRESHOLD_RATIO = 0.10\n",
        "TARGET_CLASS_IDS = [2, 5, 7]\n",
        "\n",
        "MODEL_PATH = model_file_path\n",
        "VIDEO_PATH = video_file_path\n",
        "\n",
        "# --- 出力パス設定 (変更・追加) ---\n",
        "# ローカル（Colab環境）の一時的な出力ディレクトリ\n",
        "LOCAL_BASE_OUTPUT_DIR = \"/content/output_data\"\n",
        "if not os.path.exists(LOCAL_BASE_OUTPUT_DIR):\n",
        "    os.makedirs(LOCAL_BASE_OUTPUT_DIR)\n",
        "\n",
        "# JPEG画像出力用サブディレクトリ\n",
        "LOCAL_JPG_OUTPUT_DIR = os.path.join(LOCAL_BASE_OUTPUT_DIR, \"frames_\" + video_type + \"_\" + str(CONFIDENCE_THRESHOLD))\n",
        "if not os.path.exists(LOCAL_JPG_OUTPUT_DIR):\n",
        "    os.makedirs(LOCAL_JPG_OUTPUT_DIR)\n",
        "\n",
        "OUTPUT_VIDEO_PATH = os.path.join(LOCAL_BASE_OUTPUT_DIR, \"output_parking_detection_\" + video_type + \"_\" + str(CONFIDENCE_THRESHOLD) +  \".mp4\")\n",
        "CSV_PATH = os.path.join(LOCAL_BASE_OUTPUT_DIR, \"parking_log_\" + video_type + \"_\" + str(CONFIDENCE_THRESHOLD) + \".csv\")\n",
        "\n",
        "# Google Drive上の出力先ディレクトリ (実行IDなどでユニークにすると良い)\n",
        "# ここではvideo_typeとconfidenceでサブフォルダを作成\n",
        "if DRIVE_OUTPUT_BASE_DIR:\n",
        "    DRIVE_SPECIFIC_OUTPUT_DIR = os.path.join(DRIVE_OUTPUT_BASE_DIR, f\"output_{video_type}_conf{CONFIDENCE_THRESHOLD}_{int(time.time())}\")\n",
        "    if not os.path.exists(DRIVE_SPECIFIC_OUTPUT_DIR):\n",
        "        os.makedirs(DRIVE_SPECIFIC_OUTPUT_DIR)\n",
        "    print(f\"Google Drive output will be saved to: {DRIVE_SPECIFIC_OUTPUT_DIR}\")\n",
        "else:\n",
        "    DRIVE_SPECIFIC_OUTPUT_DIR = None\n",
        "    print(\"Google Drive not mounted. Output will only be saved locally in Colab.\")\n",
        "# --------------------------------"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vchqP5x4eXhn",
        "outputId": "6eb2532a-b514-4ec0-c4f2-89184df24178"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google Drive output will be saved to: /content/drive/MyDrive/ParkingDetectionOutput/output_Akihabara_conf0.15_1748781133\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def run_parking_detection():\n",
        "    # 1. モデルのロード\n",
        "    print(f\"Loading model from {MODEL_PATH}...\")\n",
        "    try:\n",
        "        model = YOLO(MODEL_PATH)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading YOLO model: {e}\")\n",
        "        return\n",
        "\n",
        "    print(\"Model loaded successfully.\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"CUDA is available. Using GPU.\")\n",
        "        model.to('cuda')\n",
        "    else:\n",
        "        print(\"CUDA not available. Using CPU.\")\n",
        "\n",
        "    # 2. 動画の読み込みと設定\n",
        "    cap = cv2.VideoCapture(VIDEO_PATH)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: Could not open video {VIDEO_PATH}\")\n",
        "        return\n",
        "\n",
        "    video_fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    video_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    video_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    if video_fps > 0 and FRAMES_PER_SECOND_TO_PROCESS > 0:\n",
        "        frame_skip_interval = max(1, int(round(video_fps / FRAMES_PER_SECOND_TO_PROCESS)))\n",
        "    else:\n",
        "        frame_skip_interval = 1\n",
        "        if FRAMES_PER_SECOND_TO_PROCESS <=0:\n",
        "            print(f\"Warning: FRAMES_PER_SECOND_TO_PROCESS ({FRAMES_PER_SECOND_TO_PROCESS}) is invalid.\")\n",
        "\n",
        "    print(f\"Video Info: FPS={video_fps:.2f}, Size=({video_width}x{video_height}), SkipInterval={frame_skip_interval}\")\n",
        "    print(f\"Using CONFIDENCE_THRESHOLD: {CONFIDENCE_THRESHOLD}\")\n",
        "    print(f\"Using MOVEMENT_THRESHOLD_RATIO: {MOVEMENT_THRESHOLD_RATIO}\")\n",
        "    print(f\"Processed frames will be saved to: {LOCAL_JPG_OUTPUT_DIR}\")\n",
        "\n",
        "\n",
        "    output_fps = FRAMES_PER_SECOND_TO_PROCESS\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out_video = cv2.VideoWriter(OUTPUT_VIDEO_PATH, fourcc, output_fps, (video_width, video_height))\n",
        "\n",
        "    # 3. データ構造の初期化\n",
        "    tracked_vehicles = {}\n",
        "    parking_log_list = []\n",
        "\n",
        "    frame_counter = 0\n",
        "    processed_frame_count = 0\n",
        "    last_processed_time_sec = 0.0\n",
        "\n",
        "    print(\"Starting video processing...\")\n",
        "    start_time_processing = time.time()\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame_original = cap.read() # 描画前のオリジナルフレームを保持\n",
        "        if not ret:\n",
        "            print(\"End of video or error reading frame.\")\n",
        "            break\n",
        "\n",
        "        current_frame_time_msec = cap.get(cv2.CAP_PROP_POS_MSEC)\n",
        "        current_frame_time_sec = current_frame_time_msec / 1000.0\n",
        "\n",
        "        if current_frame_time_sec > PROCESS_DURATION_SECONDS:\n",
        "            print(f\"Reached processing duration limit ({PROCESS_DURATION_SECONDS}s).\")\n",
        "            break\n",
        "\n",
        "        # フレームをコピーして描画用に使用\n",
        "        frame_to_draw = frame_original.copy()\n",
        "\n",
        "        if frame_counter % frame_skip_interval == 0:\n",
        "            processed_frame_count += 1\n",
        "            last_processed_time_sec = current_frame_time_sec\n",
        "\n",
        "            # 4. 検知と追跡 (YOLOv8)\n",
        "            results = model.track(frame_original, persist=True, classes=TARGET_CLASS_IDS, conf=CONFIDENCE_THRESHOLD, verbose=False, tracker=\"bytetrack.yaml\")\n",
        "\n",
        "            if results[0].boxes is not None and results[0].boxes.id is not None:\n",
        "                boxes = results[0].boxes.xyxy.cpu().numpy()\n",
        "                track_ids = results[0].boxes.id.cpu().numpy().astype(int)\n",
        "                confs = results[0].boxes.conf.cpu().numpy()\n",
        "\n",
        "                for i, track_id in enumerate(track_ids):\n",
        "                    x1, y1, x2, y2 = map(int, boxes[i])\n",
        "                    w, h = x2 - x1, y2 - y1\n",
        "                    cx, cy = (x1 + x2) / 2, (y1 + y2) / 2\n",
        "                    current_conf = confs[i]\n",
        "\n",
        "                    if track_id not in tracked_vehicles:\n",
        "                        tracked_vehicles[track_id] = {\n",
        "                            'positions': deque(maxlen=int(FRAMES_PER_SECOND_TO_PROCESS * (PARKING_THRESHOLD_SECONDS + 5))),\n",
        "                            'is_parking': False,\n",
        "                            'parking_start_time': None,\n",
        "                            'last_seen_time': current_frame_time_sec\n",
        "                        }\n",
        "\n",
        "                    vehicle_data = tracked_vehicles[track_id]\n",
        "                    vehicle_data['positions'].append((current_frame_time_sec, cx, cy, w, h))\n",
        "                    vehicle_data['last_seen_time'] = current_frame_time_sec\n",
        "\n",
        "                    # 5. 停車判定ロジック\n",
        "                    min_data_points_for_parking_decision = int(FRAMES_PER_SECOND_TO_PROCESS * PARKING_THRESHOLD_SECONDS)\n",
        "                    if len(vehicle_data['positions']) >= min_data_points_for_parking_decision:\n",
        "                        latest_time_in_deque = vehicle_data['positions'][-1][0]\n",
        "                        cutoff_time_for_parking_check = latest_time_in_deque - PARKING_THRESHOLD_SECONDS\n",
        "                        positions_in_window = [p for p in vehicle_data['positions'] if p[0] >= cutoff_time_for_parking_check]\n",
        "\n",
        "                        if len(positions_in_window) >= min_data_points_for_parking_decision * 0.8:\n",
        "                            first_pos_data = positions_in_window[0]\n",
        "                            max_dist_sq = 0\n",
        "                            sum_w, sum_h = 0,0\n",
        "                            for pos_data_idx, pos_data in enumerate(positions_in_window):\n",
        "                                dist_sq = (pos_data[1] - first_pos_data[1])**2 + (pos_data[2] - first_pos_data[2])**2\n",
        "                                if dist_sq > max_dist_sq: max_dist_sq = dist_sq\n",
        "                                sum_w += pos_data[3]; sum_h += pos_data[4]\n",
        "                            max_movement_distance = np.sqrt(max_dist_sq)\n",
        "                            num_pos_in_window = len(positions_in_window)\n",
        "                            avg_w = sum_w / num_pos_in_window if num_pos_in_window > 0 else 0\n",
        "                            avg_h = sum_h / num_pos_in_window if num_pos_in_window > 0 else 0\n",
        "                            movement_threshold = min(avg_w, avg_h) * MOVEMENT_THRESHOLD_RATIO if min(avg_w, avg_h) > 0 else float('inf')\n",
        "                            is_currently_still = max_movement_distance < movement_threshold\n",
        "\n",
        "                            if not vehicle_data['is_parking'] and is_currently_still:\n",
        "                                vehicle_data['is_parking'] = True\n",
        "                                vehicle_data['parking_start_time'] = positions_in_window[0][0]\n",
        "                            elif vehicle_data['is_parking'] and not is_currently_still:\n",
        "                                vehicle_data['is_parking'] = False\n",
        "                                parking_end_time = current_frame_time_sec\n",
        "                                if vehicle_data['parking_start_time'] is not None:\n",
        "                                    actual_parking_duration = parking_end_time - vehicle_data['parking_start_time']\n",
        "                                    if actual_parking_duration >= PARKING_THRESHOLD_SECONDS:\n",
        "                                        parking_log_list.append({'car_id': track_id, 'start_time': round(vehicle_data['parking_start_time'], 2), 'end_time': round(parking_end_time, 2)})\n",
        "                                vehicle_data['parking_start_time'] = None\n",
        "\n",
        "                    # 6. 結果の描画 (frame_to_draw に対して行う)\n",
        "                    color = (0, 255, 0)\n",
        "                    text = f\"ID:{track_id} C:{current_conf:.2f}\"\n",
        "                    if vehicle_data['is_parking']:\n",
        "                        color = (0, 0, 255)\n",
        "                        text += f\" (P:{vehicle_data['parking_start_time']:.0f}s)\"\n",
        "                    cv2.rectangle(frame_to_draw, (x1, y1), (x2, y2), color, 2)\n",
        "                    cv2.putText(frame_to_draw, text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "\n",
        "            cv2.putText(frame_to_draw, f\"Time: {current_frame_time_sec:.2f}s\", (10, 30),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "            # MP4動画にフレームを書き込み\n",
        "            out_video.write(frame_to_draw)\n",
        "\n",
        "            # JPEG画像としてフレームを保存 (追加)\n",
        "            # ファイル名は processed_frame_count (1から始まる連番) を使用\n",
        "            jpg_filename = os.path.join(LOCAL_JPG_OUTPUT_DIR, f\"frame_{processed_frame_count:04d}.jpg\")\n",
        "            cv2.imwrite(jpg_filename, frame_to_draw)\n",
        "\n",
        "        frame_counter += 1\n",
        "        if frame_counter % (10 * frame_skip_interval) == 0: # 10処理フレームごとに進捗表示\n",
        "             elapsed_time = time.time() - start_time_processing\n",
        "             print(f\"Processed {processed_frame_count} frames (video time: {current_frame_time_sec:.2f}s). Saved {processed_frame_count} JPGs. Elapsed: {elapsed_time:.2f}s\")\n",
        "\n",
        "    print(\"Video processing finished.\")\n",
        "    print(f\"Total frames processed and saved as JPG: {processed_frame_count}\")\n",
        "\n",
        "    # 7. ループ終了後の処理 (停車ログの最終処理)\n",
        "    for track_id, data in tracked_vehicles.items():\n",
        "        if data['is_parking'] and data['parking_start_time'] is not None:\n",
        "            parking_end_time = last_processed_time_sec\n",
        "            actual_parking_duration = parking_end_time - data['parking_start_time']\n",
        "            if actual_parking_duration >= PARKING_THRESHOLD_SECONDS:\n",
        "                 parking_log_list.append({'car_id': track_id, 'start_time': round(data['parking_start_time'], 2), 'end_time': round(parking_end_time, 2)})\n",
        "\n",
        "    # 8. CSVファイルへの記録\n",
        "    if parking_log_list:\n",
        "        parking_df = pd.DataFrame(parking_log_list)\n",
        "        parking_df.sort_values(by=['car_id', 'start_time'], inplace=True)\n",
        "        parking_df.to_csv(CSV_PATH, index=False)\n",
        "        print(f\"Parking log saved locally to {CSV_PATH}\")\n",
        "        # print(parking_df.head())\n",
        "    else:\n",
        "        print(\"No parking events to log.\")\n",
        "        pd.DataFrame(columns=['car_id', 'start_time', 'end_time']).to_csv(CSV_PATH, index=False) # 空のCSVを作成\n",
        "\n",
        "    # 9. リソース解放\n",
        "    cap.release()\n",
        "    out_video.release()\n",
        "    if 'google.colab' in str(get_ipython()): # Colab環境以外での cv2.destroyAllWindows() エラーを回避\n",
        "      cv2.destroyAllWindows()\n",
        "    print(f\"Output video saved locally to {OUTPUT_VIDEO_PATH}\")\n",
        "    print(\"Local cleanup complete.\")\n",
        "\n",
        "    # 10. Google Driveへのファイル保存 (追加)\n",
        "    if DRIVE_SPECIFIC_OUTPUT_DIR:\n",
        "        print(f\"\\nStarting to copy files to Google Drive: {DRIVE_SPECIFIC_OUTPUT_DIR}\")\n",
        "        try:\n",
        "            # MP4動画のコピー\n",
        "            if os.path.exists(OUTPUT_VIDEO_PATH):\n",
        "                shutil.copy(OUTPUT_VIDEO_PATH, os.path.join(DRIVE_SPECIFIC_OUTPUT_DIR, os.path.basename(OUTPUT_VIDEO_PATH)))\n",
        "                print(f\"Copied MP4 to Drive: {os.path.basename(OUTPUT_VIDEO_PATH)}\")\n",
        "\n",
        "            # CSVファイルのコピー\n",
        "            if os.path.exists(CSV_PATH):\n",
        "                shutil.copy(CSV_PATH, os.path.join(DRIVE_SPECIFIC_OUTPUT_DIR, os.path.basename(CSV_PATH)))\n",
        "                print(f\"Copied CSV to Drive: {os.path.basename(CSV_PATH)}\")\n",
        "\n",
        "            # JPEG画像群のディレクトリごとコピー\n",
        "            if os.path.exists(LOCAL_JPG_OUTPUT_DIR) and os.listdir(LOCAL_JPG_OUTPUT_DIR): # ディレクトリが存在し、空でない場合\n",
        "                drive_jpg_dir_name = os.path.basename(LOCAL_JPG_OUTPUT_DIR)\n",
        "                shutil.copytree(LOCAL_JPG_OUTPUT_DIR, os.path.join(DRIVE_SPECIFIC_OUTPUT_DIR, drive_jpg_dir_name))\n",
        "                print(f\"Copied JPG frames directory to Drive: {drive_jpg_dir_name}\")\n",
        "            elif not os.listdir(LOCAL_JPG_OUTPUT_DIR):\n",
        "                 print(f\"JPG directory {LOCAL_JPG_OUTPUT_DIR} is empty. Not copying to Drive.\")\n",
        "\n",
        "\n",
        "            print(\"All files copied to Google Drive successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error copying files to Google Drive: {e}\")\n",
        "    else:\n",
        "        print(\"\\nGoogle Drive not configured. Skipping copy to Drive.\")\n",
        "        print(\"You can download files manually from the Colab environment's /content/output_data/ directory.\")\n",
        "\n",
        "# このセルを実行する前に、model_file_path, video_file_path, video_type が定義されている必要があります。\n",
        "if __name__ == \"__main__\" and 'google.colab' in str(get_ipython()):\n",
        "    if os.path.exists(MODEL_PATH) and os.path.exists(VIDEO_PATH):\n",
        "        run_parking_detection()\n",
        "    else:\n",
        "        print(\"Please ensure model and video files are uploaded to /content/ and that 'video_type' is defined.\")\n",
        "        print(\"Example: model_file_path = '/content/yolov8m.pt'; video_file_path = '/content/your_video.mp4'; video_type = 'test_run'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLm6gcqPetr2",
        "outputId": "2fd4b127-7f57-44ec-8b0a-39d8c8c4e237"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from /content/yolov8m.pt...\n",
            "Model loaded successfully.\n",
            "CUDA not available. Using CPU.\n",
            "Video Info: FPS=30.00, Size=(640x360), SkipInterval=30\n",
            "Using CONFIDENCE_THRESHOLD: 0.15\n",
            "Using MOVEMENT_THRESHOLD_RATIO: 0.1\n",
            "Processed frames will be saved to: /content/output_data/frames_Akihabara_0.15\n",
            "Starting video processing...\n",
            "Processed 10 frames (video time: 9.97s). Saved 10 JPGs. Elapsed: 10.55s\n",
            "Processed 20 frames (video time: 19.97s). Saved 20 JPGs. Elapsed: 20.51s\n",
            "Processed 30 frames (video time: 29.97s). Saved 30 JPGs. Elapsed: 28.89s\n",
            "Processed 40 frames (video time: 39.97s). Saved 40 JPGs. Elapsed: 38.82s\n",
            "Processed 50 frames (video time: 49.97s). Saved 50 JPGs. Elapsed: 48.69s\n",
            "Processed 60 frames (video time: 59.97s). Saved 60 JPGs. Elapsed: 57.12s\n",
            "Processed 70 frames (video time: 69.97s). Saved 70 JPGs. Elapsed: 67.12s\n",
            "Processed 80 frames (video time: 79.97s). Saved 80 JPGs. Elapsed: 77.16s\n",
            "Processed 90 frames (video time: 89.97s). Saved 90 JPGs. Elapsed: 87.59s\n",
            "Processed 100 frames (video time: 99.97s). Saved 100 JPGs. Elapsed: 96.75s\n",
            "Processed 110 frames (video time: 109.97s). Saved 110 JPGs. Elapsed: 106.84s\n",
            "Processed 120 frames (video time: 119.97s). Saved 120 JPGs. Elapsed: 116.53s\n",
            "Processed 130 frames (video time: 129.97s). Saved 130 JPGs. Elapsed: 125.13s\n",
            "Processed 140 frames (video time: 139.97s). Saved 140 JPGs. Elapsed: 135.12s\n",
            "Processed 150 frames (video time: 149.97s). Saved 150 JPGs. Elapsed: 144.97s\n",
            "Processed 160 frames (video time: 159.97s). Saved 160 JPGs. Elapsed: 153.47s\n",
            "Processed 170 frames (video time: 169.97s). Saved 170 JPGs. Elapsed: 163.39s\n",
            "Processed 180 frames (video time: 179.97s). Saved 180 JPGs. Elapsed: 173.31s\n",
            "Reached processing duration limit (180s).\n",
            "Video processing finished.\n",
            "Total frames processed and saved as JPG: 181\n",
            "Parking log saved locally to /content/output_data/parking_log_Akihabara_0.15.csv\n",
            "Output video saved locally to /content/output_data/output_parking_detection_Akihabara_0.15.mp4\n",
            "Local cleanup complete.\n",
            "\n",
            "Starting to copy files to Google Drive: /content/drive/MyDrive/ParkingDetectionOutput/output_Akihabara_conf0.15_1748781133\n",
            "Copied MP4 to Drive: output_parking_detection_Akihabara_0.15.mp4\n",
            "Copied CSV to Drive: parking_log_Akihabara_0.15.csv\n",
            "Copied JPG frames directory to Drive: frames_Akihabara_0.15\n",
            "All files copied to Google Drive successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#旧ver\n",
        "import cv2\n",
        "import torch\n",
        "from ultralytics import YOLO\n",
        "import pandas as pd\n",
        "import time\n",
        "import os\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow # Colabでの画像表示用\n",
        "\n",
        "#YOLOのConfidenceを設定できるように改良\n",
        "\n",
        "# 0. 設定値\n",
        "PROCESS_DURATION_SECONDS = 180  # 動画の処理対象時間（秒）\n",
        "FRAMES_PER_SECOND_TO_PROCESS = 1  # 1秒あたりに処理するフレーム数\n",
        "PARKING_THRESHOLD_SECONDS = 10  # 停車とみなす時間（秒）\n",
        "\n",
        "# --- フィードバック対応パラメータ ---\n",
        "# YOLO検出時のConfidence閾値 (0.0 から 1.0 の間。低いほど検出しやすいが誤検出も増える)\n",
        "# デフォルトはYOLOライブラリ側で0.25程度。少し下げてみます。\n",
        "CONFIDENCE_THRESHOLD = 0.15 # 例: 15%\n",
        "\n",
        "# 位置変動の閾値（バウンディングボックスの短辺の割合）\n",
        "# この値を大きくすると、多少の揺れや微動があっても停車と判定されやすくなる\n",
        "# デフォルト: 0.05 (5%) -> 少し緩めてみます\n",
        "MOVEMENT_THRESHOLD_RATIO = 0.10 # 例: 10%\n",
        "# ---------------------------------\n",
        "\n",
        "MODEL_PATH = model_file_path\n",
        "VIDEO_PATH = video_file_path\n",
        "OUTPUT_VIDEO_PATH = \"/content/output_parking_detection_\" + video_type + \"_\" + str(CONFIDENCE_THRESHOLD) +  \".mp4\"\n",
        "CSV_PATH = \"/content/parking_log_\" + video_type + \"_\" + str(CONFIDENCE_THRESHOLD) + \".csv\"\n",
        "\n",
        "TARGET_CLASS_IDS = [2, 5, 7] # 検知対象クラスID: 2 (car), 5 (bus), 7 (truck)\n",
        "\n",
        "def run_parking_detection():\n",
        "    # 1. モデルのロード\n",
        "    print(f\"Loading model from {MODEL_PATH}...\")\n",
        "    try:\n",
        "        model = YOLO(MODEL_PATH)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading YOLO model: {e}\")\n",
        "        return\n",
        "\n",
        "    print(\"Model loaded successfully.\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"CUDA is available. Using GPU.\")\n",
        "        model.to('cuda')\n",
        "    else:\n",
        "        print(\"CUDA not available. Using CPU.\")\n",
        "\n",
        "    # 2. 動画の読み込みと設定\n",
        "    cap = cv2.VideoCapture(VIDEO_PATH)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: Could not open video {VIDEO_PATH}\")\n",
        "        return\n",
        "\n",
        "    video_fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    video_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    video_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    if video_fps > 0 and FRAMES_PER_SECOND_TO_PROCESS > 0:\n",
        "        frame_skip_interval = max(1, int(round(video_fps / FRAMES_PER_SECOND_TO_PROCESS)))\n",
        "    else:\n",
        "        frame_skip_interval = 1\n",
        "        if FRAMES_PER_SECOND_TO_PROCESS <=0:\n",
        "            print(f\"Warning: FRAMES_PER_SECOND_TO_PROCESS ({FRAMES_PER_SECOND_TO_PROCESS}) is invalid. Defaulting to processing all frames selected by skip interval.\")\n",
        "\n",
        "    print(f\"Video Info: FPS={video_fps:.2f}, Size=({video_width}x{video_height}), SkipInterval={frame_skip_interval}\")\n",
        "    print(f\"Using CONFIDENCE_THRESHOLD: {CONFIDENCE_THRESHOLD}\")\n",
        "    print(f\"Using MOVEMENT_THRESHOLD_RATIO: {MOVEMENT_THRESHOLD_RATIO}\")\n",
        "\n",
        "    output_fps = FRAMES_PER_SECOND_TO_PROCESS\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out_video = cv2.VideoWriter(OUTPUT_VIDEO_PATH, fourcc, output_fps, (video_width, video_height))\n",
        "\n",
        "    # 3. データ構造の初期化\n",
        "    tracked_vehicles = {}\n",
        "    parking_log_list = []\n",
        "\n",
        "    frame_counter = 0\n",
        "    processed_frame_count = 0\n",
        "    last_processed_time_sec = 0.0\n",
        "\n",
        "    print(\"Starting video processing...\")\n",
        "    start_time_processing = time.time()\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            print(\"End of video or error reading frame.\")\n",
        "            break\n",
        "\n",
        "        current_frame_time_msec = cap.get(cv2.CAP_PROP_POS_MSEC)\n",
        "        current_frame_time_sec = current_frame_time_msec / 1000.0\n",
        "\n",
        "        if current_frame_time_sec > PROCESS_DURATION_SECONDS:\n",
        "            print(f\"Reached processing duration limit ({PROCESS_DURATION_SECONDS}s).\")\n",
        "            break\n",
        "\n",
        "        if frame_counter % frame_skip_interval == 0:\n",
        "            processed_frame_count += 1\n",
        "            last_processed_time_sec = current_frame_time_sec\n",
        "\n",
        "            # 4. 検知と追跡 (YOLOv8)\n",
        "            results = model.track(frame, persist=True, classes=TARGET_CLASS_IDS, conf=CONFIDENCE_THRESHOLD, verbose=False, tracker=\"bytetrack.yaml\")\n",
        "\n",
        "            if results[0].boxes is not None and results[0].boxes.id is not None:\n",
        "                boxes = results[0].boxes.xyxy.cpu().numpy()\n",
        "                track_ids = results[0].boxes.id.cpu().numpy().astype(int)\n",
        "                confs = results[0].boxes.conf.cpu().numpy() # Confidenceの値を取得\n",
        "\n",
        "                for i, track_id in enumerate(track_ids): # enumerateを使用してインデックスiを取得\n",
        "                    x1, y1, x2, y2 = map(int, boxes[i])\n",
        "                    w, h = x2 - x1, y2 - y1\n",
        "                    cx, cy = (x1 + x2) / 2, (y1 + y2) / 2\n",
        "                    current_conf = confs[i] # 現在の検出のConfidence\n",
        "\n",
        "                    if track_id not in tracked_vehicles:\n",
        "                        tracked_vehicles[track_id] = {\n",
        "                            'positions': deque(maxlen=int(FRAMES_PER_SECOND_TO_PROCESS * (PARKING_THRESHOLD_SECONDS + 5))),\n",
        "                            'is_parking': False,\n",
        "                            'parking_start_time': None,\n",
        "                            'last_seen_time': current_frame_time_sec\n",
        "                        }\n",
        "\n",
        "                    vehicle_data = tracked_vehicles[track_id]\n",
        "                    vehicle_data['positions'].append((current_frame_time_sec, cx, cy, w, h))\n",
        "                    vehicle_data['last_seen_time'] = current_frame_time_sec\n",
        "\n",
        "                    # 5. 停車判定ロジック (変更なし)\n",
        "                    min_data_points_for_parking_decision = int(FRAMES_PER_SECOND_TO_PROCESS * PARKING_THRESHOLD_SECONDS)\n",
        "                    if len(vehicle_data['positions']) >= min_data_points_for_parking_decision:\n",
        "                        latest_time_in_deque = vehicle_data['positions'][-1][0]\n",
        "                        cutoff_time_for_parking_check = latest_time_in_deque - PARKING_THRESHOLD_SECONDS\n",
        "                        positions_in_window = [p for p in vehicle_data['positions'] if p[0] >= cutoff_time_for_parking_check]\n",
        "\n",
        "                        if len(positions_in_window) >= min_data_points_for_parking_decision * 0.8:\n",
        "                            first_pos_data = positions_in_window[0]\n",
        "                            max_dist_sq = 0\n",
        "                            sum_w, sum_h = 0,0\n",
        "                            for pos_data_idx, pos_data in enumerate(positions_in_window):\n",
        "                                dist_sq = (pos_data[1] - first_pos_data[1])**2 + (pos_data[2] - first_pos_data[2])**2\n",
        "                                if dist_sq > max_dist_sq: max_dist_sq = dist_sq\n",
        "                                sum_w += pos_data[3]; sum_h += pos_data[4]\n",
        "                            max_movement_distance = np.sqrt(max_dist_sq)\n",
        "                            num_pos_in_window = len(positions_in_window)\n",
        "                            avg_w = sum_w / num_pos_in_window if num_pos_in_window > 0 else 0\n",
        "                            avg_h = sum_h / num_pos_in_window if num_pos_in_window > 0 else 0\n",
        "                            movement_threshold = min(avg_w, avg_h) * MOVEMENT_THRESHOLD_RATIO if min(avg_w, avg_h) > 0 else float('inf')\n",
        "                            is_currently_still = max_movement_distance < movement_threshold\n",
        "\n",
        "                            if not vehicle_data['is_parking'] and is_currently_still:\n",
        "                                vehicle_data['is_parking'] = True\n",
        "                                vehicle_data['parking_start_time'] = positions_in_window[0][0]\n",
        "                            elif vehicle_data['is_parking'] and not is_currently_still:\n",
        "                                vehicle_data['is_parking'] = False\n",
        "                                parking_end_time = current_frame_time_sec\n",
        "                                if vehicle_data['parking_start_time'] is not None:\n",
        "                                    actual_parking_duration = parking_end_time - vehicle_data['parking_start_time']\n",
        "                                    if actual_parking_duration >= PARKING_THRESHOLD_SECONDS:\n",
        "                                        parking_log_list.append({'car_id': track_id, 'start_time': round(vehicle_data['parking_start_time'], 2), 'end_time': round(parking_end_time, 2)})\n",
        "                                vehicle_data['parking_start_time'] = None\n",
        "\n",
        "                    # 6. 結果の描画 (Confidence表示を追加)\n",
        "                    color = (0, 255, 0) # 緑: 通常\n",
        "                    # IDとConfidenceをラベルに表示\n",
        "                    text = f\"ID:{track_id} C:{current_conf:.2f}\"\n",
        "                    if vehicle_data['is_parking']:\n",
        "                        color = (0, 0, 255) # 赤: 停車中\n",
        "                        # 停車情報を短縮して表示 (P: Parking start time)\n",
        "                        text += f\" (P:{vehicle_data['parking_start_time']:.0f}s)\"\n",
        "\n",
        "                    cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
        "                    cv2.putText(frame, text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2) # フォントサイズを少し調整(0.6->0.5)\n",
        "\n",
        "            cv2.putText(frame, f\"Time: {current_frame_time_sec:.2f}s\", (10, 30),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "            out_video.write(frame)\n",
        "\n",
        "        frame_counter += 1\n",
        "        if frame_counter % (10 * frame_skip_interval) == 0:\n",
        "             elapsed_time = time.time() - start_time_processing\n",
        "             print(f\"Processed {processed_frame_count} frames. Video time: {current_frame_time_sec:.2f}s. Elapsed: {elapsed_time:.2f}s\")\n",
        "\n",
        "    print(\"Video processing finished.\")\n",
        "    print(f\"Total frames processed: {processed_frame_count}\")\n",
        "\n",
        "    for track_id, data in tracked_vehicles.items():\n",
        "        if data['is_parking'] and data['parking_start_time'] is not None:\n",
        "            parking_end_time = last_processed_time_sec\n",
        "            actual_parking_duration = parking_end_time - data['parking_start_time']\n",
        "            if actual_parking_duration >= PARKING_THRESHOLD_SECONDS:\n",
        "                 parking_log_list.append({'car_id': track_id, 'start_time': round(data['parking_start_time'], 2), 'end_time': round(parking_end_time, 2)})\n",
        "\n",
        "    if parking_log_list:\n",
        "        parking_df = pd.DataFrame(parking_log_list)\n",
        "        parking_df.sort_values(by=['car_id', 'start_time'], inplace=True)\n",
        "        parking_df.to_csv(CSV_PATH, index=False)\n",
        "        print(f\"Parking log saved to {CSV_PATH}\")\n",
        "        print(parking_df.head())\n",
        "    else:\n",
        "        print(\"No parking events to log.\")\n",
        "        pd.DataFrame(columns=['car_id', 'start_time', 'end_time']).to_csv(CSV_PATH, index=False)\n",
        "\n",
        "    cap.release()\n",
        "    out_video.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    print(f\"Output video saved to {OUTPUT_VIDEO_PATH}\")\n",
        "    print(\"Cleanup complete.\")\n",
        "\n",
        "if __name__ == \"__main__\" and 'google.colab' in str(get_ipython()):\n",
        "    if os.path.exists(MODEL_PATH) and os.path.exists(VIDEO_PATH):\n",
        "        run_parking_detection()\n",
        "    else:\n",
        "        print(\"Please ensure model and video files are uploaded to /content/ and run Cell 2 again.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "8FtHM4U48W_t",
        "outputId": "323db6e1-81a6-4954-b114-f5f20d101531"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from /content/yolov8m.pt...\n",
            "Model loaded successfully.\n",
            "CUDA not available. Using CPU.\n",
            "Video Info: FPS=30.00, Size=(640x360), SkipInterval=30\n",
            "Using CONFIDENCE_THRESHOLD: 0.15\n",
            "Using MOVEMENT_THRESHOLD_RATIO: 0.1\n",
            "Starting video processing...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-c6060dcae6d7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'google.colab'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_PATH\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVIDEO_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0mrun_parking_detection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Please ensure model and video files are uploaded to /content/ and run Cell 2 again.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-c6060dcae6d7>\u001b[0m in \u001b[0;36mrun_parking_detection\u001b[0;34m()\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;31m# 4. 検知と追跡 (YOLOv8)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpersist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTARGET_CLASS_IDS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCONFIDENCE_THRESHOLD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bytetrack.yaml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboxes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36mtrack\u001b[0;34m(self, source, stream, persist, **kwargs)\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m1\u001b[0m  \u001b[0;31m# batch-size 1 for tracking in videos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mode\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"track\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     def val(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprompts\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"set_prompts\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# for SAM-type models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_prompts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_cli\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_cli\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m     def track(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/predictor.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# merge list of Result into one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_cli\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mgenerator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m# Issuing `None` to a generator fires it up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/predictor.py\u001b[0m in \u001b[0;36mstream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    328\u001b[0m                 \u001b[0;31m# Inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mprofilers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m                     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mpreds\u001b[0m  \u001b[0;31m# yield embedding tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/predictor.py\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32melse\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         )\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvisualize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpre_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/nn/autobackend.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, im, augment, visualize, embed, **kwargs)\u001b[0m\n\u001b[1;32m    634\u001b[0m         \u001b[0;31m# PyTorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpt\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn_module\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maugment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvisualize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m         \u001b[0;31m# TorchScript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/nn/tasks.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# for cases of training and validating while training.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/nn/tasks.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_augment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_predict_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/nn/tasks.py\u001b[0m in \u001b[0;36m_predict_once\u001b[0;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mprofile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_profile_one_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# save output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/upsampling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         return F.interpolate(\n\u001b[0m\u001b[1;32m    173\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[1;32m   4647\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupsample_nearest1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_factors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4648\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"nearest\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4649\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupsample_nearest2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_factors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4650\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"nearest\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4651\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupsample_nearest3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_factors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "erkyuydlhlZz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}